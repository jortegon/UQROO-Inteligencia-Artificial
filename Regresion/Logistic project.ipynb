{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Logistic Regression Project\n",
    "\n",
    "For this project you will use the Iris dataset of the package Scikit-Learn. \n",
    "\n",
    "For simplicity, they are used the first two dimensions (sepal length and width) of the iris dataset to train a logistic-regression classifiers.\n",
    "\n",
    "A straight line must be seen in the plot, showing how logistic regression attempts to draw a straight line that will best clasify the first class from the others in the dataset.\n",
    "\n",
    "## Your work to do\n",
    "\n",
    "Your work is to implement tow functions:\n",
    "\n",
    "1. costFunction: a function that using a logistic model computes the cost for the theta coefficients and its gradient\n",
    "\n",
    "2. predict: a function to compute the hypothesis of your logistic model returning a label\n",
    "\n",
    "Also, you should use `scipy.optimize` to optimize your model, i.e., to fit the model to your data. \n",
    "\n",
    "Finally, after an analysis from the comparison, you have to answer the questions.\n",
    "\n",
    "Extra points: Separate the three classes.\n",
    "## Comparison\n",
    "\n",
    "The coefficients are calculated, in order to compare with scikit-learn logistic model.\n",
    "\n",
    "The plot could look like, you may only separate the blue class from others.\n",
    "\n",
    "![logistic plot](https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_logistic_001.png)\n",
    "\n",
    "## Iris dataset\n",
    "For reference on the dataset you may visit [https://scikit-learn.org/stable/datasets/toy_dataset.html]\n",
    "\n",
    "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher’s paper. Note that it’s the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points.\n"
    "This is perhaps the best known database to be found in the pattern recognition literature. Fisher’s paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.\n",
    "\n",
    "Data Set Characteristics:\n",
    "\n",
    "    Number of Instances  |  150 (50 in each of three classes)\n",
    "    Number of Attributes |  4 numeric, predictive attributes and the class\n",
    "    Attribute Information|  sepal length in cm\n",
    "                            sepal width in cm\n",
    "                            petal length in cm\n",
    "                            petal width in cm\n",
    "                            class:\n",
    "                                Iris-Setosa\n",
    "                                Iris-Versicolour\n",
    "                                Iris-Virginica\n",
    "    Class Distribution   |  33.3% for each of 3 classes.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "Y = np.where(iris.target < 0.5,1,0) # we only take the first class.\n",
    "Y = Y.reshape(Y.size,1)\n"
    "\n",
    "#X = (X - X.mean())/X.std()\n",
    "#  Setup the data matrix appropriately, and add ones for the intercept term\n",
    "m, n = X.shape\n",
    "\n",
    "# Add intercept term to x and X_test\n",
    "X = np.concatenate((np.ones((m,1)), X), axis=1) # Add a column of ones to x\n",
    "\n",
    "# Initialize fitting parameters\n",
    "initial_theta = np.zeros(n + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "        SIGMOID Compute sigmoid functoon\n",
    "        J = SIGMOID(z) computes the sigmoid of z.\n",
    "    '''\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    if not isinstance(z, np.ndarray):\n",
    "        z = np.array(z)\n",
    "    one = np.ones(z.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Compute the sigmoid of each value of z (z can be a matrix,\n",
    "    #               vector or scalar).\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic model \n",
    "# Cost function and Gradient\n",
    "# If you get a \"nan\" cost while optimizing it may be due a mulplication of zero by inf. You may avoid this error by clamping the value of log\n",
    "#    log_h = np.log(h)\n",
    "#    log_1h = np.log(1-h)\n",
    "#    log_h[np.isinf(log_h)]=np.finfo(float).max\n",
    "#    log_1h[np.isinf(log_1h)]=np.finfo(float).max\n",
    "\n",
    "def costFunction(theta, X, y):\n",
    "    cost = np.zeros(1)\n",
    "    grad = np.zeros(theta.shape)\n",
    "\n",
    "    #You have to complete this function\n",
    "\n",
    "    return cost.item(), grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model using the training sets\n",
    "\n",
    "# Using scipy optimize the cost function\n",
    "options={'maxiter':1000, 'gtol':1e-5,'disp': True}\n",
    "solution = minimize(costFunction, initial_theta, args=(X, Y),jac=True, options=options)\n",
    "cost = solution['fun']\n",
    "theta = solution['x']\n",
    "# Print theta to screen\n",
    "print('Cost at theta found by minimize function: ', cost)\n",
    "# The coefficients\n",
    "print('Coefficients (theta): \\n', theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't forget to take your values to 0 and 1\n",
    "#    p = np.where(h > 0.5,1,0)\n",
    "\n",
    "def predict(theta, X):\n",
    "    '''Predict whether the label\n",
    "    is 0 or 1 using learned logistic\n",
    "    regression parameters '''\n",
    "\n",
    "    m, n = X.shape\n",
    "    p = np.zeros((m, 1))\n",
    "\n",
    "    #You have to complete this function\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "y_min, y_max = X[:, 2].min() - .5, X[:, 2].max() + .5\n",
    "h = .02  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = predict(theta, np.c_[np.ones(xx.size),xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 1], X[:, 2], edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compare your result with that from Scikit-Learn\n",
    "Xlr = iris.data[:, :2]  # we only take the first two features.\n",
    "Y = iris.target\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "logreg.fit(Xlr, Y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = Xlr[:, 0].min() - .5, Xlr[:, 0].max() + .5\n",
    "y_min, y_max = Xlr[:, 1].min() - .5, Xlr[:, 1].max() + .5\n",
    "h = .02  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(Xlr[:, 0], Xlr[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coefficients\n",
    "print('Coefficients: \\n', logreg.intercept_[0], logreg.coef_[0])\n",
    "# The mean squared error"
   ]
  },
  {
   "source": [
    "## Questions\n",
    "\n",
    "1. Did you perform a data preprocessing (Standardization, Scaling, Normalization, Categorical Encoding or Imputation)?\n",
    "2. Are the model parameters theta the same as the intercept and coefficients from scikit-learn? Why do you think that this happens? They represent (almost) the same line?\n",
    "3. Do you have to modify the two functions that you implemented in order to fit a model that uses more than two features of the dataset?\n",
    "4. How you can use one-hot encoding to classify the three classes?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
