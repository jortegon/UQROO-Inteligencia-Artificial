{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Regularized Logistic Regression\n",
    "In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.\n",
    "Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "!wget https://raw.githubusercontent.com/jortegon/UQROO-Inteligencia-Artificial/master/Regresion/ex2data2.txt\n",
    "data = np.genfromtxt('ex2data2.txt', delimiter=',') #load the data\n",
    "X = data[:, :2] # two scores\n",
    "y = data[:, 2] # admited \n",
    "m = len(y)     # number of samples \n",
    "\n",
    "#Just for indexing purposes\n",
    "X = X.reshape(m,2)\n",
    "y = y.reshape(m,1)\n",
    "\n",
    "#see the values for y\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(X, y):\n",
    "    '''PLOTDATA Plots the data points X and y into a new figure \n",
    "       PLOTDATA(x,y) plots the data points with + for the positive examples\n",
    "       and o for the negative examples. X is assumed to be a Mx2 matrix.\n",
    "    '''\n",
    "\n",
    "    # Find Indices of Positive and Negative Examples \n",
    "    pos = np.where(y==1)\n",
    "    neg = np.where(y == 0)\n",
    "    # Plot Examples \n",
    "    posplot = plt.scatter(X[pos, 0], X[pos, 1], marker='+', label='y=1') \n",
    "    negplot = plt.scatter(X[neg, 0], X[neg, 1], marker='o', label='y=0') \n",
    "    # Labels and Legend\n",
    "    plt.xlabel('Microchip Test 1')\n",
    "    plt.ylabel('Microchip Test 2')\n",
    "\n",
    "    # Specified in plot order\n",
    "    plt.legend()"
   ]
  },
  {
   "source": [
    "plotData is used to generate a figure where the axes are the two test scores, and the positive (y = 1, accepted) and negative (y = 0, rejected) examples are shown with different markers. This figure shows that our dataset cannot be separated into positive and negative examples by a straight-line through the plot. Therefore, a straight-forward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData(X,y)"
   ]
  },
  {
   "source": [
    "## Feature mapping\n",
    "One way to fit the data better is to create more features from each data point. In the provided function mapFeature.m, we will map the features into all polynomial terms of x1 and x2 up to the sixth power.\n",
    "\n",
    "$$\n",
    "mapFeature(x) = \\left[ \\begin{array} 1 \\\\ x_1 \\\\ x_2 \\\\ x_1^2 \\\\ x_1 x_2 \\\\ x_2^2 \\\\ x_1^3 \\\\ \\vdots \\\\ x_1 x_2^5 \\\\ x^6_2 \\end{array} \\right]\n",
    "$$\n",
    "As a result of this mapping, our vector of two features (the scores on two QA tests) has been transformed into a 28-dimensional vector. A logistic regression classifier trained on this higher-dimension feature vector will have a more complex decision boundary and will appear nonlinear when drawn in our 2-dimensional plot.\n",
    "\n",
    "While the feature mapping allows us to build a more expressive classifier, it also more susceptible to overfitting. In the next parts of the exercise, you will implement regularized logistic regression to fit the data and also see for yourself how regularization can help combat the overfitting problem.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapFeature(X1, X2):\n",
    "    ''' MAPFEATURE Feature mapping function to polynomial features\n",
    "\n",
    "        MAPFEATURE(X1, X2) maps the two input features\n",
    "        to quadratic features used in the regularization exercise.\n",
    "\n",
    "        Returns a new feature array with more features, comprising of \n",
    "        X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..\n",
    "\n",
    "        Inputs X1, X2 must be the same size\n",
    "    '''\n",
    "\n",
    "\n",
    "    degree = 6\n",
    "    m = 1 if isinstance(X1,float) else len(X1)\n",
    "    out = [np.ones(m)]\n",
    "    for i in range(degree+1):\n",
    "        for j in range(i+1):\n",
    "            out.append( (X1**(i-j)) * (X2**j) )\n",
    "    return np.array(out).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that mapFeature also adds a column of ones for us, so the intercept\n",
    "# term is handled\n",
    "X_ext = mapFeature(X[:,0], X[:,1])\n",
    "print ( X_ext[1:3,:] )\n",
    "\n",
    "# Initialize fitting parameters\n",
    "initial_theta = np.zeros(X_ext.shape[1])\n",
    "\n",
    "# Set regularization parameter lambda to 1\n",
    "lambda_t = 1\n"
   ]
  },
  {
   "source": [
    "## Cost function and gradient\n",
    "\n",
    "Now you will implement code to compute the cost function and gradient for regularized logistic regression. Recall that the regularized cost function in logistic regression is\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum^m_{i=1} \\left[ −y^{(i)}\\log(h_\\theta(x^{(i)})) − (1−y^{(i)})\\log(1−h_\\theta(x^{(i)})) \\right] + \\frac{\\lambda}{2m}\\sum_{j=1}^n \\theta_j^2 $$\n",
    "\n",
    "Note that you should not regularize the parameter $\\theta_0$. The gradient of the cost function is a vector where the $j^{th}$ element is defined as follows:\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial\\theta_j} = \\frac{1}{m}\\sum^m_{i=1}(h_\\theta(x^{(i)}) − y^{(i)})x^{(i)}_j ~~~~~ \\mathrm{for} ~ j = 0 $$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial\\theta_j} = \\left( \\frac{1}{m}\\sum^m_{i=1}(h_\\theta(x^{(i)}) − y^{(i)})x^{(i)}_j \\right) + \\frac{\\lambda}{m}\\theta_j  ~~~~~ \\mathrm{for} ~ j \\geq 1 $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "        SIGMOID Compute sigmoid functoon\n",
    "        J = SIGMOID(z) computes the sigmoid of z.\n",
    "    '''\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    if not isinstance(z, np.ndarray):\n",
    "        z = np.array(z)\n",
    "    one = np.ones(z.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Compute the sigmoid of each value of z (z can be a matrix,\n",
    "    #               vector or scalar).\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunctionReg(theta, X, y, lambda_t):\n",
    "    '''\n",
    "        COSTFUNCTIONREG Compute cost and gradient for logistic regression\n",
    "        J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost of using theta         as the parameter for logistic regression and the gradient of the cost\n",
    "        w.r.t. to the parameters.\n",
    "    '''\n",
    "\n",
    "    # Initialize some useful values\n",
    "    m = len(y) # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Compute the cost of a particular choice of theta\n",
    "    #               You should set J to the cost.\n",
    "    #               Compute the partial derivatives and set grad to the partial\n",
    "    #               derivatives of the cost w.r.t. each parameter in theta\n",
    "    #\n",
    "    # Note: grad should have the same dimensions as theta\n",
    "\n",
    "    return J.item(), grad\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display initial cost and gradient for regularized logistic\n",
    "# regression\n",
    "cost , grad = costFunctionReg(initial_theta, X_ext, y, lambda_t)\n",
    "\n",
    "# You should see that the cost is about 0.693.\n",
    "print('Cost at initial theta (zeros): ', cost)\n",
    "print(grad[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using scipy optimize the cost function\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "#  Run fmin_bfgs to obtain the optimal theta\n",
    "#  This function will return theta and the cost \n",
    "options={'maxiter':400,'gtol': 1e-8, 'disp': True}\n",
    "solution = minimize(costFunctionReg, initial_theta, args=(X_ext, y,lambda_t),jac=True, options=options)\n",
    "cost = solution['fun']\n",
    "theta = solution['x']\n",
    "# Print theta to screen\n",
    "print('Cost at theta found by minimize function: ', cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDecisionBoundary(theta, X, y):\n",
    "    '''\n",
    "    PLOTDECISIONBOUNDARY Plots the data points X and y into a new figure with\n",
    "    the decision boundary defined by theta\n",
    "       PLOTDECISIONBOUNDARY(theta, X,y) plots the data points with + for the \n",
    "       positive examples and o for the negative examples. X is assumed to be \n",
    "       a Mx3 matrix, where the first column is an all-ones column for the \n",
    "       intercept.\n",
    "    '''\n",
    "    plotData(X, y)\n",
    "\n",
    "    # Here is the grid range\n",
    "    u = np.linspace(-1, 1.5, 50)\n",
    "    v = np.linspace(-1, 1.5, 50)\n",
    "    \n",
    "    uv = [ [u_i, v_j] for u_i in u for v_j in v ]\n",
    "    uv = np.array(uv)\n",
    "    mapuv = mapFeature(uv[:,0],uv[:,1])\n",
    "    z = np.dot(mapuv,theta)\n",
    "    z = z.reshape(50,50)\n",
    "    #print(z)\n",
    "\n",
    "    # Plot z = 0\n",
    "    # Notice you need to specify the range 0\n",
    "    plt.contour(u, v, z.T, 0, linewidth=2)\n",
    "\n",
    "    \n",
    "    # Legend, specific for the exercise\n",
    "    plt.legend()\n",
    "\n",
    "def predict(theta, X):\n",
    "    '''Predict whether the label\n",
    "    is 0 or 1 using learned logistic\n",
    "    regression parameters '''\n",
    "\n",
    "    h = np.dot(mapFeature(X[:,0],X[:,1]),theta)\n",
    "\n",
    "    p = np.where(h > 0.5,1,0)\n",
    "\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute accuracy on our training set\n",
    "p = predict(theta, X);\n",
    "print('Train Accuracy: ', np.count_nonzero(p == y.reshape(m))/m)\n",
    "\n",
    "#Plot Boundary\n",
    "plotDecisionBoundary(theta, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
