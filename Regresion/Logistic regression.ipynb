{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Logistic Regression\n",
    "In this exercise, you will implement logistic regression and get to see it work on data.\n",
    "\n",
    "In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university.\n",
    "Suppose that you are the administrator of a university department and you want to determine each applicant’s chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant’s scores on two exams and the admissions decision.\n",
    "Your task is to build a classification model that estimates an applicant’s probability of admission based the scores from those two exams. \n",
    "\n",
    "## Visualizing the data\n",
    "Before starting to implement any learning algorithm, it is always good to visualize the data if possible. The code will load the data and display it on a 2-dimensional plot by calling the function plotData.\n",
    "plotData displays a figure, where the axes are the two exam scores, and the positive and negative examples are shown with different markers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "!wget https://raw.githubusercontent.com/jortegon/UQROO-Inteligencia-Artificial/master/Regresion/ex2data1.txt\n",
    "data = np.genfromtxt('ex2data1.txt', delimiter=',') #load the data\n",
    "X = data[:, :2] # two scores\n",
    "y = data[:, 2] # admited \n",
    "m = len(y)     # number of samples \n",
    "\n",
    "#Just for indexing purposes\n",
    "X = X.reshape(m,2)\n",
    "y = y.reshape(m,1)\n",
    "\n",
    "#see the values for y\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(X, y):\n",
    "    '''PLOTDATA Plots the data points X and y into a new figure \n",
    "       PLOTDATA(x,y) plots the data points with + for the positive examples\n",
    "       and o for the negative examples. X is assumed to be a Mx2 matrix.\n",
    "    '''\n",
    "\n",
    "    # Find Indices of Positive and Negative Examples \n",
    "    pos = np.where(y==1)\n",
    "    neg = np.where(y == 0)\n",
    "    # Plot Examples \n",
    "    posplot = plt.scatter(X[pos, 0], X[pos, 1], marker='+', label='Admitted') \n",
    "    negplot = plt.scatter(X[neg, 0], X[neg, 1], marker='o', label='Not admitted') \n",
    "    # Labels and Legend\n",
    "    plt.xlabel('Exam 1 score')\n",
    "    plt.ylabel('Exam 2 score')\n",
    "\n",
    "    # Specified in plot order\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData(X,y)"
   ]
  },
  {
   "source": [
    "## sigmoid function\n",
    "Before you start with the actual cost function, recall that the logistic regression hypothesis is defined as:\n",
    "$$ h_\\theta(x) = g(\\theta^T x), $$\n",
    "where function $g$ is the sigmoid function. The sigmoid function is defined as:\n",
    "$$ g(z) = \\frac{1}{1+e^{−z}} $$\n",
    "Your first step is to implement this function so it can be called by the rest of your program. When you are finished, try testing a few values by calling sigmoid(x). For large positive values of x, the sigmoid should be close to 1, while for large negative values, the sigmoid should be close to 0. Evaluating sigmoid(0) should give you exactly 0.5. Your code should also work with vectors and matrices. For a matrix, your function should perform the sigmoid function on every element."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "        SIGMOID Compute sigmoid functoon\n",
    "        J = SIGMOID(z) computes the sigmoid of z.\n",
    "    '''\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    if not isinstance(z, np.ndarray):\n",
    "        z = np.array(z)\n",
    "    one = np.ones(z.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Compute the sigmoid of each value of z (z can be a matrix,\n",
    "    #               vector or scalar).\n",
    "\n",
    "    g = #complete#\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sigmoid(100))\n",
    "print(sigmoid(-100))\n",
    "print(sigmoid(0))\n",
    "print(sigmoid([100, -100, 0]))"
   ]
  },
  {
   "source": [
    "## Gradient Descent\n",
    "In this part, you will fit the logistic regression parameters θ to our dataset using gradient descent.\n",
    "\n",
    "### Update Equations\n",
    "The objective of linear regression is to minimize the cost function\n",
    "\n",
    "$$ J(\\theta)= \\frac{1}{m} \\sum_{i=1}^{m} [-y^{(i)} \\log (h_\\theta(x^{(i)})) − (1-y^{(i)})\\log (1-h_\\theta(x^{(i)}))] $$\n",
    "\n",
    "where the hypothesis $h_\\theta(x)$ is given by the logistic model\n",
    "$$ h_\\theta(x) = g(\\theta^T x). $$\n",
    "\n",
    "the gradient of the cost is a vector of the same length as $\\theta$ where the $j^{th}$ element (for $j = 0,1,\\ldots,n$) is defined as follows:\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m (h\\theta(x^{(i)}) − y^{(i)})x^{(i)}_j $$\n",
    "\n",
    "Note that while this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of $h_\\theta(x)$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(theta, X, y):\n",
    "    '''\n",
    "        COSTFUNCTION Compute cost and gradient for logistic regression\n",
    "        J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the\n",
    "        parameter for logistic regression and the gradient of the cost\n",
    "        w.r.t. to the parameters.\n",
    "    '''\n",
    "\n",
    "    # Initialize some useful values\n",
    "    m = len(y) # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    grad = np.zeros(theta.shape);\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Compute the cost of a particular choice of theta\n",
    "    #               You should set J to the cost.\n",
    "    #               Compute the partial derivatives and set grad to the partial\n",
    "    #               derivatives of the cost w.r.t. each parameter in theta\n",
    "    #\n",
    "    # Note: grad should have the same dimensions as theta\n",
    "    \n",
    "\n",
    "    return J.item(), grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Setup the data matrix appropriately, and add ones for the intercept term\n",
    "m, n = X.shape\n",
    "\n",
    "# Add intercept term to x and X_test\n",
    "X = np.concatenate((np.ones((m,1)), X), axis=1) # Add a column of ones to x\n",
    "\n",
    "# Initialize fitting parameters\n",
    "initial_theta = np.zeros(n + 1)\n",
    "\n",
    "# Compute and display initial cost and gradient\n",
    "cost, grad = costFunction(initial_theta, X, y)\n",
    "\n",
    "print('Cost at initial theta (zeros): ', cost)\n",
    "print('Gradient at initial theta (zeros): ', grad);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using scipy optimize the cost function\n",
    "from scipy.optimize import fmin_tnc\n",
    "\n",
    "#  Run fmin_bfgs to obtain the optimal theta\n",
    "#  This function will return theta and the cost \n",
    "solution = fmin_tnc(costFunction, initial_theta, args=(X, y),maxfun=400)\n",
    "theta = solution[0]\n",
    "cost, grad = costFunction(theta, X, y)\n",
    "# Print theta to screen\n",
    "print('Cost at theta found by fminunc: ', cost)\n",
    "print('theta: ', theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDecisionBoundary(theta, X, y):\n",
    "    '''\n",
    "    PLOTDECISIONBOUNDARY Plots the data points X and y into a new figure with\n",
    "    the decision boundary defined by theta\n",
    "       PLOTDECISIONBOUNDARY(theta, X,y) plots the data points with + for the \n",
    "       positive examples and o for the negative examples. X is assumed to be \n",
    "       a Mx3 matrix, where the first column is an all-ones column for the \n",
    "       intercept.\n",
    "    '''\n",
    "    plotData(X[:,1:], y)\n",
    "\n",
    "    # Only need 2 points to define a line, so choose two endpoints\n",
    "    plot_x = np.array([np.min(X[:,1])-2,  np.max(X[:,1])+2])\n",
    "\n",
    "    # Calculate the decision boundary line\n",
    "    plot_y = (-1/theta[2])*(theta[1]*plot_x + theta[0])\n",
    "\n",
    "    # Plot, and adjust axes for better viewing\n",
    "    plt.plot(plot_x, plot_y,'r',label='Decision Boundary')\n",
    "    \n",
    "    # Legend, specific for the exercise\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Boundary\n",
    "plotDecisionBoundary(theta, X, y)"
   ]
  },
  {
   "source": [
    "## Evaluating logistic regression\n",
    "After learning the parameters, you can use the model to predict whether a particular student will be admitted. For a student with an Exam 1 score of 45 and an Exam 2 score of 85, you should expect to see an admission probability of 0.776.\n",
    "Another way to evaluate the quality of the parameters we have found is to see how well the learned model predicts on our training set. In this part, your task is to complete the code. The predict function will produce “1” or “0” predictions given a dataset and a learned parameter vector $\\theta$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, X):\n",
    "    '''Predict whether the label\n",
    "    is 0 or 1 using learned logistic\n",
    "    regression parameters '''\n",
    "    m, n = X.shape\n",
    "    p = np.zeros((m, 1))\n",
    "\n",
    "    h = sigmoid(X.dot(theta.T))\n",
    "\n",
    "    p = np.where(h > 0.5,1,0)\n",
    "\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Predict probability for a student with score 45 on exam 1 \n",
    "#  and score 85 on exam 2 \n",
    "scores = np.array([1, 45, 85])\n",
    "prob = sigmoid( scores.dot(theta))\n",
    "print('For a student with scores 45 and 85, we predict an admission probability of ', prob)\n",
    "\n",
    "# Compute accuracy on our training set\n",
    "p = predict(theta, X);\n",
    "\n",
    "print('Train Accuracy: ', np.count_nonzero(p == y.reshape(m))/m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}