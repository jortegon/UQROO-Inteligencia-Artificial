{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd0082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7",
   "display_name": "Python 3.8.6 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jortegon/UQROO-Inteligencia-Artificial/blob/master/NeuralNets/oneVsAll.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "source": [
    "# Multi-class Classification\n",
    "\n",
    "In this exercise, you will implement one-vs-all logistic regression and neural networks to recognize handwritten digits. \n",
    "\n",
    "All the information you need for solving this assignment is in this notebook, and all the code you will be implementing will take place within this notebook. \n",
    "\n",
    "Before we begin with the exercises, we need to import all libraries required for this programming exercise. We will be using numpy for all arrays and matrix operations, matplotlib for plotting, and scipy for scientific and numerical computation functions and tools. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "from scipy import optimize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a helper function to display handwritten digits from MNITS dataset\n",
    "def displayData(X, example_width=None):\n",
    "    \"\"\" %DISPLAYDATA Display 2D data in a nice grid\n",
    "   [h, display_array] = DISPLAYDATA(X, example_width) displays 2D data\n",
    "   stored in X in a nice grid. \n",
    "    \"\"\"\n",
    "    # Compute rows, cols\n",
    "    m, n = X.shape\n",
    "\n",
    "    # Set example_width automatically if not passed in\n",
    "    if example_width is None:\n",
    "        example_width = int(np.round(np.sqrt(n)))\n",
    "\n",
    "    example_height = n / example_width\n",
    "\n",
    "    # Compute number of items to display\n",
    "    display_rows = int(np.floor(np.sqrt(m)))\n",
    "    display_cols = int(np.ceil(m / display_rows))\n",
    "\n",
    "    fig, ax_array = plt.subplots(display_rows, display_cols)\n",
    "    fig.subplots_adjust(wspace=0.025, hspace=0.025)\n",
    "\n",
    "    ax_array = [ax_array] if m == 1 else ax_array.ravel()\n",
    "\n",
    "    for i, ax in enumerate(ax_array):\n",
    "        ax.imshow(X[i].reshape(example_width, example_width, order='F'),\n",
    "                    cmap='Greys', extent=[0, 1, 0, 1])\n",
    "        ax.axis('off')"
   ]
  },
  {
   "source": [
    "## Multi-class Classification\n",
    "\n",
    "For this exercise, you will use logistic regression and neural networks to recognize handwritten digits (from 0 to 9). Automated handwritten digit recognition is widely used today - from recognizing zip codes (postal codes)\n",
    "on mail envelopes to recognizing amounts written on bank checks. This exercise will show you how the methods you have learned can be used for this classification task.\n",
    "\n",
    "In the first part of the exercise, you will extend your previous implementation of logistic regression and apply it to one-vs-all classification.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "You are given a data set in `ex3data1.mat` that contains 5000 training examples of handwritten digits (This is a subset of the [MNIST](http://yann.lecun.com/exdb/mnist) handwritten digit dataset). The `.mat` format means that that the data has been saved in a native Octave/MATLAB matrix format, instead of a text (ASCII) format like a csv-file. We use the `.mat` format here because this is the dataset provided in the MATLAB version of this assignment. Fortunately, python provides mechanisms to load MATLAB native format using the `loadmat` function within the `scipy.io` module. This function returns a python dictionary with keys containing the variable names within the `.mat` file. \n",
    "\n",
    "There are 5000 training examples in `ex3data1.mat`, where each training example is a 20 pixel by 20 pixel grayscale image of the digit. Each pixel is represented by a floating point number indicating the grayscale intensity at that location. The 20 by 20 grid of pixels is “unrolled” into a 400-dimensional vector. Each of these training examples becomes a single row in our data matrix `X`. This gives us a 5000 by 400 matrix `X` where every row is a training example for a handwritten digit image.\n",
    "\n",
    "$$ X = \\begin{bmatrix} - \\: (x^{(1)})^T \\: - \\\\ -\\: (x^{(2)})^T \\:- \\\\ \\vdots \\\\ - \\: (x^{(m)})^T \\:-  \\end{bmatrix} $$\n",
    "\n",
    "The second part of the training set is a 5000-dimensional vector `y` that contains labels for the training set. \n",
    "We start the exercise by first loading the dataset. Execute the cell below, you do not need to write any code here."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_layer_size  = 400  # 20x20 Input Images of Digits\n",
    "num_labels = 10          # 10 labels, from 1 to 10   \n",
    "                          # (note that we have mapped \"0\" to label 10)\n",
    "\n",
    "\"\"\" =========== Part 1: Loading and Visualizing Data =============\n",
    "  We start the exercise by first loading and visualizing the dataset. \n",
    "  You will be working with a dataset that contains handwritten digits.\n",
    "\"\"\"\n",
    "\n",
    "# Load Training Data\n",
    "\n",
    "!wget https://raw.githubusercontent.com/jortegon/UQROO-Inteligencia-Artificial/master/NeuralNets/ex3data1.mat\n",
    "mat = scipy.io.loadmat('ex3data1.mat') # training data \n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "m = len(X)\n",
    "\n",
    "# Randomly select 100 data points to display\n",
    "rand_indices = np.random.choice(m, 100, replace=False)\n",
    "sel = X[rand_indices, :]\n",
    "\n",
    "displayData(sel)"
   ]
  },
  {
   "source": [
    "### Vectorizing Logistic Regression\n",
    "\n",
    "You will be using multiple one-vs-all logistic regression models to build a multi-class classifier. Since there are 10 classes, you will need to train 10 separate logistic regression classifiers. To make this training efficient, it is important to ensure that your code is well vectorized. In this section, you will implement a vectorized version of logistic regression that does not employ any `for` loops. You can use your code in the previous exercise as a starting point for this exercise. \n",
    "\n",
    "To test your vectorized logistic regression, we will use custom data as defined in the following cell."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Vectorizing the cost function \n",
    "\n",
    "We will begin by writing a vectorized version of the cost function. Recall that in (unregularized) logistic regression, the cost function is\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)} \\log \\left( h_\\theta\\left( x^{(i)} \\right) \\right) - \\left(1 - y^{(i)} \\right) \\log \\left(1 - h_\\theta \\left( x^{(i)} \\right) \\right) \\right] $$\n",
    "\n",
    "To compute each element in the summation, we have to compute $h_\\theta(x^{(i)})$ for every example $i$, where $h_\\theta(x^{(i)}) = g(\\theta^T x^{(i)})$ and $g(z) = \\frac{1}{1+e^{-z}}$ is the sigmoid function. It turns out that we can compute this quickly for all our examples by using matrix multiplication. Let us define $X$ and $\\theta$ as\n",
    "\n",
    "$$ X = \\begin{bmatrix} - \\left( x^{(1)} \\right)^T - \\\\ - \\left( x^{(2)} \\right)^T - \\\\ \\vdots \\\\ - \\left( x^{(m)} \\right)^T - \\end{bmatrix} \\qquad \\text{and} \\qquad \\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix} $$\n",
    "\n",
    "Then, by computing the matrix product $X\\theta$, we have: \n",
    "\n",
    "$$ X\\theta = \\begin{bmatrix} - \\left( x^{(1)} \\right)^T\\theta - \\\\ - \\left( x^{(2)} \\right)^T\\theta - \\\\ \\vdots \\\\ - \\left( x^{(m)} \\right)^T\\theta - \\end{bmatrix} = \\begin{bmatrix} - \\theta^T x^{(1)}  - \\\\ - \\theta^T x^{(2)} - \\\\ \\vdots \\\\ - \\theta^T x^{(m)}  - \\end{bmatrix} $$\n",
    "\n",
    "In the last equality, we used the fact that $a^Tb = b^Ta$ if $a$ and $b$ are vectors. This allows us to compute the products $\\theta^T x^{(i)}$ for all our examples $i$ in one line of code.\n",
    "\n",
    "#### Vectorizing the gradient\n",
    "\n",
    "Recall that the gradient of the (unregularized) logistic regression cost is a vector where the $j^{th}$ element is defined as\n",
    "\n",
    "$$ \\frac{\\partial J }{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( \\left( h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x_j^{(i)} \\right) $$\n",
    "\n",
    "To vectorize this operation over the dataset, we start by writing out all the partial derivatives explicitly for all $\\theta_j$,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial J}{\\partial \\theta_0} \\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_1} \\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_n}\n",
    "\\end{bmatrix} = &\n",
    "\\frac{1}{m} \\begin{bmatrix}\n",
    "\\sum_{i=1}^m \\left( \\left(h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x_0^{(i)}\\right) \\\\\n",
    "\\sum_{i=1}^m \\left( \\left(h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x_1^{(i)}\\right) \\\\\n",
    "\\sum_{i=1}^m \\left( \\left(h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x_2^{(i)}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\sum_{i=1}^m \\left( \\left(h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x_n^{(i)}\\right) \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "= & \\frac{1}{m} \\sum_{i=1}^m \\left( \\left(h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x^{(i)}\\right) \\\\\n",
    "= & \\frac{1}{m} X^T \\left( h_\\theta(x) - y\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$  h_\\theta(x) - y = \n",
    "\\begin{bmatrix}\n",
    "h_\\theta\\left(x^{(1)}\\right) - y^{(1)} \\\\\n",
    "h_\\theta\\left(x^{(2)}\\right) - y^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "h_\\theta\\left(x^{(m)}\\right) - y^{(m)} \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "\n",
    "Note that $x^{(i)}$ is a vector, while $h_\\theta\\left(x^{(i)}\\right) - y^{(i)}$  is a scalar (single number).\n",
    "To understand the last step of the derivation, let $\\beta_i = (h_\\theta\\left(x^{(m)}\\right) - y^{(m)})$ and\n",
    "observe that:\n",
    "\n",
    "$$ \n",
    "\\sum_i \\beta_ix^{(i)} = \\begin{bmatrix} \n",
    "| & | & & | \\\\\n",
    "x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\\\\n",
    "| & | & & | \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\beta_1 \\\\\n",
    "\\beta_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_m\n",
    "\\end{bmatrix} = x^T \\beta\n",
    "$$\n",
    "\n",
    "where the values $\\beta_i = \\left( h_\\theta(x^{(i)} - y^{(i)} \\right)$.\n",
    "\n",
    "The expression above allows us to compute all the partial derivatives\n",
    "without any loops. If you are comfortable with linear algebra, we encourage you to work through the matrix multiplications above to convince yourself that the vectorized version does the same computations. \n",
    "\n",
    "#### Vectorizing regularized logistic regression\n",
    "\n",
    "After you have implemented vectorization for logistic regression, you will now\n",
    "add regularization to the cost function. Recall that for regularized logistic\n",
    "regression, the cost function is defined as\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)} \\log \\left(h_\\theta\\left(x^{(i)} \\right)\\right) - \\left( 1 - y^{(i)} \\right) \\log\\left(1 - h_\\theta \\left(x^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 $$\n",
    "\n",
    "Note that you should not be regularizing $\\theta_0$ which is used for the bias term.\n",
    "Correspondingly, the partial derivative of regularized logistic regression cost for $\\theta_j$ is defined as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta\\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)}  & \\text{for } j = 0 \\\\\n",
    "& \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\left( \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta\\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} \\right) + \\frac{\\lambda}{m} \\theta_j & \\text{for } j  \\ge 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now modify your code in lrCostFunction to account for regularization. Once again, you should not put any loops into your code.\n",
    "\n",
    "**Debugging Tip:** Vectorizing code can sometimes be tricky. One common strategy for debugging is to print out the sizes of the matrices you are working with using the `shape` property of `numpy` arrays. For example, given a data matrix $X$ of size $100 \\times 20$ (100 examples, 20 features) and $\\theta$, a vector with size $20$, you can observe that `np.dot(X, theta)` is a valid multiplication operation, while `np.dot(theta, X)` is not. Furthermore, if you have a non-vectorized version of your code, you can compare the output of your vectorized code and non-vectorized code to make sure that they produce the same outputs.\n",
    "\n",
    "**python/numpy Tip:** When implementing the vectorization for regularized logistic regression, you might often want to only sum and update certain elements of $\\theta$. In `numpy`, you can index into the matrices to access and update only certain elements. For example, A[:, 3:5]\n",
    "= B[:, 1:3] will replaces the columns with index 3 to 5 of A with the columns with index 1 to 3 from B. To select columns (or rows) until the end of the matrix, you can leave the right hand side of the colon blank. For example, A[:, 2:] will only return elements from the $3^{rd}$ to last columns of $A$. If you leave the left hand size of the colon blank, you will select elements from the beginning of the matrix. For example, A[:, :2] selects the first two columns, and is equivalent to A[:, 0:2]. In addition, you can use negative indices to index arrays from the end. Thus, A[:, :-1] selects all columns of A except the last column, and A[:, -5:] selects the $5^{th}$ column from the end to the last column. Thus, you could use this together with the sum and power ($^{**}$) operations to compute the sum of only the elements you are interested in (e.g., `np.sum(z[1:]**2)`). In the starter code, `lrCostFunction`, we have also provided hints on yet another possible method computing the regularized gradient.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" ============ Part 2: Vectorize Logistic Regression ============\n",
    "In this part of the exercise, you will reuse your logistic regression\n",
    "code from the last exercise. You task here is to make sure that your\n",
    "regularized logistic regression implementation is vectorized. After\n",
    "that, you will implement one-vs-all classification for the handwritten\n",
    "digit dataset.\n",
    "\"\"\"\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    return the sigmoid of z\n",
    "    \"\"\"\n",
    "    return 1/ (1 + np.exp(-z))\n",
    "\n",
    "def lrCostFunction(theta, X, y, Lambda):\n",
    "    \"\"\"\n",
    "    Computes the cost of using theta as the parameter for regularized\n",
    "    logistic regression and the gradient of the cost w.r.t. to the parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array_like\n",
    "        Logistic regression parameters. A vector with shape (n, ). n is the number of features including any intercept.  \n",
    "    \n",
    "    X : array_like\n",
    "        The data set with shape (m x n). m is the number of examples, and n is the number of features (including intercept).\n",
    "    \n",
    "    y : array_like\n",
    "        The data labels. A vector with shape (m, ).\n",
    "    \n",
    "    lambda_ : float\n",
    "        The regularization parameter. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the regularized cost function. \n",
    "    \n",
    "    grad : array_like\n",
    "        A vector of shape (n, ) which is the gradient of the cost\n",
    "        function with respect to theta, at the current values of theta.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the cost of a particular choice of theta. You should set J to the cost.\n",
    "    Compute the partial derivatives and set grad to the partial\n",
    "    derivatives of the cost w.r.t. each parameter in theta\n",
    "    \n",
    "    Hint 1\n",
    "    ------\n",
    "    The computation of the cost function and gradients can be efficiently\n",
    "    vectorized. For example, consider the computation\n",
    "    \n",
    "        sigmoid(X @ theta)\n",
    "    \n",
    "    Each row of the resulting matrix will contain the value of the prediction\n",
    "    for that example. You can make use of this to vectorize the cost function\n",
    "    and gradient computations. \n",
    "    \n",
    "    Hint 2\n",
    "    ------\n",
    "    When computing the gradient of the regularized cost function, there are\n",
    "    many possible vectorized solutions, but one solution looks like:\n",
    "    \n",
    "        grad = (unregularized gradient for logistic regression)\n",
    "        temp = theta \n",
    "        temp[0] = 0   # because we don't add anything for j = 0\n",
    "        grad = grad + YOUR_CODE_HERE (using the temp variable)\n",
    "    \n",
    "    \"\"\"\n",
    "    #Initialize some useful values\n",
    "    m = y.size\n",
    "    \n",
    "    # convert labels to ints if their type is bool\n",
    "    if y.dtype == bool:\n",
    "        y = y.astype(int)\n",
    "    \n",
    "    \n",
    "    return regCost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_t = np.array([-2,-1,1,2]).reshape(4,1)\n",
    "X_t =np.array([np.linspace(0.1,1.5,15)]).reshape(3,5).T\n",
    "X_t = np.hstack((np.ones((5,1)), X_t))\n",
    "y_t = np.array([1,0,1,0,1]).reshape(5,1)\n",
    "J, grad = lrCostFunction(theta_t, X_t, y_t, 3)\n",
    "print(\"Cost:\",J,\"Expected cost: 2.534819\")\n",
    "print(\"Gradients:\\n\",grad,\"\\nExpected gradients:\\n 0.146561\\n -0.548558\\n 0.724722\\n 1.398003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneVsAll(X, y, num_labels, Lambda):\n",
    " \"\"\"\n",
    "    Trains num_labels logistic regression classifiers and returns\n",
    "    each of these classifiers in a matrix all_theta, where the i-th\n",
    "    row of all_theta corresponds to the classifier for label i.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The input dataset of shape (m x n). m is the number of \n",
    "        data points, and n is the number of features. Note that we \n",
    "        do not assume that the intercept term (or bias) is in X, however\n",
    "        we provide the code below to add the bias term to X. \n",
    "    \n",
    "    y : array_like\n",
    "        The data labels. A vector of shape (m, ).\n",
    "    \n",
    "    num_labels : int\n",
    "        Number of possible labels.\n",
    "    \n",
    "    lambda_ : float\n",
    "        The logistic regularization parameter.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    all_theta : array_like\n",
    "        The trained parameters for logistic regression for each class.\n",
    "        This is a matrix of shape (K x n+1) where K is number of classes\n",
    "        (ie. `numlabels`) and n is number of features without the bias.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    You should complete the following code to train `num_labels`\n",
    "    logistic regression classifiers with regularization parameter `lambda_`. \n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    You can use y == c to obtain a vector of 1's and 0's that tell you\n",
    "    whether the ground truth is true/false for this class.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    For this assignment, we recommend using `scipy.optimize.minimize(method='CG')`\n",
    "    to optimize the cost function. It is okay to use a for-loop \n",
    "    (`for c in range(num_labels):`) to loop over the different classes.\n",
    "    \n",
    "    Example Code\n",
    "    ------------\n",
    "    \n",
    "        # Set Initial theta\n",
    "        initial_theta = np.zeros(n + 1)\n",
    "      \n",
    "        # Set options for minimize\n",
    "        options = {'maxiter': 50}\n",
    "    \n",
    "        # Run minimize to obtain the optimal theta. This function will \n",
    "        # return a class object where theta is in `res.x` and cost in `res.fun`\n",
    "        res = optimize.minimize(lrCostFunction, \n",
    "                                initial_theta, \n",
    "                                (X, (y == c), lambda_), \n",
    "                                jac=True, \n",
    "                                method='TNC',\n",
    "                                options=options) \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    initial_theta = np.zeros((n+1,1))\n",
    "    all_theta = []\n",
    "  \n",
    "    return all_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = 0.1\n",
    "all_theta = oneVsAll(X, y, num_labels, Lambda)"
   ]
  },
  {
   "source": [
    "#### One-vs-all Prediction\n",
    "\n",
    "After training your one-vs-all classifier, you can now use it to predict the digit contained in a given image. For each input, you should compute the “probability” that it belongs to each class using the trained logistic regression classifiers. Your one-vs-all prediction function will pick the class for which the corresponding logistic regression classifier outputs the highest probability and return the class label (0, 1, ..., K-1) as the prediction for the input example. You should now complete the code in the function `predictOneVsAll` to use the one-vs-all classifier for making predictions. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictOneVsAll(all_theta, X):\n",
    "    \"\"\"\n",
    "    Return a vector of predictions for each example in the matrix X. \n",
    "    Note that X contains the examples in rows. all_theta is a matrix where\n",
    "    the i-th row is a trained logistic regression theta vector for the \n",
    "    i-th class. You should set p to a vector of values from 0..K-1 \n",
    "    (e.g., p = [0, 2, 0, 1] predicts classes 0, 2, 0, 1 for 4 examples) .\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    all_theta : array_like\n",
    "        The trained parameters for logistic regression for each class.\n",
    "        This is a matrix of shape (K x n+1) where K is number of classes\n",
    "        and n is number of features without the bias.\n",
    "    \n",
    "    X : array_like\n",
    "        Data points to predict their labels. This is a matrix of shape \n",
    "        (m x n) where m is number of data points to predict, and n is number \n",
    "        of features without the bias term. Note we add the bias term for X in \n",
    "        this function. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    p : array_like\n",
    "        The predictions for each data point in X. This is a vector of shape (m, ).\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Complete the following code to make predictions using your learned logistic\n",
    "    regression parameters (one-vs-all). You should set p to a vector of predictions\n",
    "    (from 0 to num_labels-1).\n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    This code can be done all vectorized using the numpy argmax function.\n",
    "    In particular, the argmax function returns the index of the max element,\n",
    "    for more information see '?np.argmax' or search online. If your examples\n",
    "    are in rows, then, you can use np.argmax(A, axis=1) to obtain the index \n",
    "    of the max for each row.\n",
    "    \"\"\"\n",
    "    m= X.shape[0]\n",
    "    X = np.hstack((np.ones((m,1)),X))\n",
    "    \n",
    "    predictions = X @ all_theta.T\n",
    "    return np.argmax(predictions,axis=1)+1"
   ]
  },
  {
   "source": [
    "Once you are done, call your `predictOneVsAll` function using the learned value of $\\theta$. You should see that the training set accuracy is about 95.1% (i.e., it classifies 95.1% of the examples in the training set correctly)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ================ Part 3: Predict for One-Vs-All ================\n",
    "  After ...\n",
    "\"\"\"\n",
    "pred = predictOneVsAll(all_theta, X)\n",
    "\n",
    "print(\"Training Set Accuracy:\",sum(pred[:,np.newaxis]==y)[0]/5000*100,\"%\")"
   ]
  }
 ]
}
