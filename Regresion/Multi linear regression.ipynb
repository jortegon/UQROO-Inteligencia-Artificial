{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Linear regression with multiple variables\n",
    "In this part, you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.\n",
    "The file ex1data2.txt contains a training set of housing prices in Port- land, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "!wget https://raw.githubusercontent.com/jortegon/UQROO-Inteligencia-Artificial/master/Regresion/ex1data2.txt\n",
    "data = np.genfromtxt('ex1data2.txt', delimiter=',') #load the data\n",
    "X = data[:, :2] # population  #all columns before column two, i.e., 0 and 1\n",
    "y = data[:, 2] # profit\n",
    "m = len(y)     # number of samples \n",
    "\n",
    "#Just for indexing purposes\n",
    "X = X.reshape(m,2)\n",
    "y = y.reshape(m,1)\n",
    "\n",
    "# Print out some data points\n",
    "print('First 10 examples from the dataset:')\n",
    "print(X[:10,:])\n",
    "print(y[:10,:])\n"
   ]
  },
  {
   "source": [
    "## Feature Normalization\n",
    "By looking at the values, note that house sizes are about 1000 times the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly.\n",
    "Your task here is to complete the code to\n",
    "• Subtract the mean value of each feature from the dataset.\n",
    "• After subtracting the mean, additionally scale (divide) the feature values by their respective “standard deviations.”"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalize(X):\n",
    "    ''' FEATURENORMALIZE Normalizes the features in X \n",
    "        FEATURENORMALIZE(X) returns a normalized version of X where\n",
    "        the mean value of each feature is 0 and the standard deviation\n",
    "        is 1. This is often a good preprocessing step to do when\n",
    "        working with learning algorithms.\n",
    "    '''\n",
    "\n",
    "    # You need to set these values correctly\n",
    "    X_norm = X;\n",
    "    mu = np.zeros((1, X.shape[1]))\n",
    "    sigma = np.zeros((1, X.shape[1]))\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: First, for each feature dimension, compute the mean\n",
    "    #               of the feature and subtract it from the dataset,\n",
    "    #               storing the mean value in mu. Next, compute the \n",
    "    #               standard deviation of each feature and divide\n",
    "    #               each feature by it's standard deviation, storing\n",
    "    #               the standard deviation in sigma. \n",
    "    #\n",
    "    #               Note that X is a matrix where each column is a \n",
    "    #               feature and each row is an example. You need \n",
    "    #               to perform the normalization separately for \n",
    "    #               each feature. \n",
    "    #\n",
    "    # Hint: You might find the 'mean' and 'std' functions useful.\n",
    " \n",
    "    return X_norm, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, mu, sigma = featureNormalize(X);\n",
    "\n",
    "# Add intercept term to X\n",
    "X = np.concatenate((np.ones((m,1)), X), axis=1) # Add a column of ones to X"
   ]
  },
  {
   "source": [
    "## Gradient Descent\n",
    "In this part, you will fit the linear regression parameters θ to our dataset using gradient descent.\n",
    "\n",
    "### Update Equations\n",
    "The objective of linear regression is to minimize the cost function\n",
    "\n",
    "$$ J(\\theta)= \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)})−y^{(i)})^2 $$\n",
    "\n",
    "where the hypothesis $h_\\theta(x)$ is given by the linear model\n",
    "$$ h_\\theta(x)=\\theta^Tx=\\theta_0 + \\theta_1x_1 $$\n",
    "Recall that the parameters of your model are the $\\theta_j$ values. These are the values you will adjust to minimize cost $J(\\theta)$. One way to do this is to use the batch gradient descent algorithm. In batch gradient descent, each iteration performs the update\n",
    "$$\\theta_j := \\theta_j − \\alpha\\frac{1}{m}\\sum_{i=1}^{m} (h_\\theta(x^{(i)}) − y^{(i)})x^{(i)} \\mathrm{(simultaneously ~ update ~} \\theta_j \\mathrm{~ for ~ all ~} j\\mathrm{).} $$\n",
    "With each step of gradient descent, your parameters $\\theta_j$ come closer to the optimal values that will achieve the lowest cost $J(\\theta)$.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCostMulti(X, y, theta):\n",
    "    ''' COMPUTECOSTMULTI Compute cost for linear regression\n",
    "        J = COMPUTECOSTMULTI(X, y, theta) computes the cost of using theta as the\n",
    "        parameter for linear regression to fit the data points in X and y\n",
    "        X, y, and theta are numpy arrays/matrices\n",
    "    '''\n",
    "\n",
    "    # Initialize some useful values\n",
    "    m = len(y) # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Compute the cost of a particular choice of theta\n",
    "    #               You should set J to the cost.\n",
    "\n",
    "  \n",
    "    return J.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentMulti(X, y, theta, alpha, num_iters):\n",
    "    '''GRADIENTDESCENTMULTI Performs gradient descent to learn theta\n",
    "        theta = GRADIENTDESENTMULTI(X, y, theta, alpha, num_iters) updates theta by \n",
    "        taking num_iters gradient steps with learning rate alpha\n",
    "    '''\n",
    "\n",
    "    # Initialize some useful values\n",
    "    m = len(y) # number of training examples\n",
    "    J_history = np.zeros((num_iters, 1))\n",
    "\n",
    "    for iter in range(num_iters):\n",
    "\n",
    "        # ====================== YOUR CODE HERE ======================\n",
    "        # Instructions: Perform a single gradient step on the parameter vector theta. \n",
    "        # Hint: While debugging, it can be useful to print out the values\n",
    "        #       of the cost function (computeCost) and gradient here.\n",
    "\n",
    "        \n",
    "        # ============================================================\n",
    "        # Save the cost J in every iteration    \n",
    "        J_history[iter] = computeCostMulti(X, y, theta)\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Choose some alpha value\n",
    "alpha = 0.01\n",
    "num_iters = 400\n",
    "\n",
    "# Init Theta and Run Gradient Descent \n",
    "theta = np.zeros((3, 1))\n",
    "theta, J_history = gradientDescentMulti(X, y, theta, alpha, num_iters)\n",
    "\n",
    "# print theta to screen\n",
    "print('Theta found by gradient descent: ')\n",
    "print(theta)\n",
    "\n",
    "# Plot convergence graph\n",
    "plt.plot(range(len(J_history)), J_history, '-b', linewidth=2)\n",
    "plt.xlabel('Number of iterations') #Set the x−axis label \n",
    "plt.ylabel('Cost J')\n",
    "\n",
    "# Estimate the price of a 1650 sq-ft, 3 br house\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "# Recall that the first column of X is all-ones. Thus, it does\n",
    "# not need to be normalized.\n",
    "house = np.array([1650,3])\n",
    "house_norm = np.divide((house-mu),sigma)\n",
    "house_norm = np.insert(house_norm,0,1)\n",
    "price = np.dot(house_norm,theta)\n",
    "\n",
    "print('Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): ', price);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}