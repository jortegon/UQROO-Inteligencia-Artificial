{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Linear Regression\n",
    "In this exercise, you will implement linear regression and get to see it work on data.\n",
    "The first part of the excercise gives you practice with Python syntax. Next, you will find the outline of a function. Modify it to return a 5 x 5 identity matrix by filling in the following code:\n",
    "A = np.eye(5);\n",
    "\n",
    "Remember: We usually use lowercase letters for vectors and uppercase letters for matrices"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "A=#put the code here#\n",
    "print(A)"
   ]
  },
  {
   "source": [
    "## Linear regression with one variable\n",
    "In this part of this exercise, you will implement linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities.\n",
    "You would like to use this data to help you select which city to expand to next.\n",
    "The file ex1data1.txt contains the dataset for our linear regression problem. The first column is the population of a city and the second column is the profit of a food truck in that city. A negative value for profit indicates a loss.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/jortegon/UQROO-Inteligencia-Artificial/master/Regresion/ex1data1.txt\n",
    "data = np.genfromtxt('ex1data1.txt', delimiter=',') #load the data\n",
    "x = data[:, 0] # population\n",
    "y = data[:, 1] # profit\n",
    "m = len(y)     # number of samples \n",
    "\n",
    "#Just for indexing purposes\n",
    "x = x.reshape(m,1)\n",
    "y = y.reshape(m,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'rx', markersize=10) #Plot the data \n",
    "plt.ylabel('Profit in $10,000s') #Set the y−axis label \n",
    "plt.xlabel('Population of City in 10,000s') # Set the x−axis label\n"
   ]
  },
  {
   "source": [
    "## Gradient Descent\n",
    "In this part, you will fit the linear regression parameters θ to our dataset using gradient descent.\n",
    "\n",
    "### Update Equations\n",
    "The objective of linear regression is to minimize the cost function\n",
    "\n",
    "$$ J(\\theta)= \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)})−y^{(i)})^2 $$\n",
    "\n",
    "where the hypothesis $h_\\theta(x)$ is given by the linear model\n",
    "$$ h_\\theta(x)=\\theta^Tx=\\theta_0 + \\theta_1x_1 $$\n",
    "Recall that the parameters of your model are the $\\theta_j$ values. These are the values you will adjust to minimize cost $J(\\theta)$. One way to do this is to use the batch gradient descent algorithm. In batch gradient descent, each iteration performs the update\n",
    "$$\\theta_j := \\theta_j − \\alpha\\frac{1}{m}\\sum_{i=1}^{m} (h_\\theta(x^{(i)}) − y^{(i)})x^{(i)} \\mathrm{(simultaneously ~ update ~} \\theta_j \\mathrm{~ for ~ all ~} j\\mathrm{).} $$\n",
    "With each step of gradient descent, your parameters $\\theta_j$ come closer to the optimal values that will achieve the lowest cost $J(\\theta)$.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X, y, theta):\n",
    "    ''' COMPUTECOST Compute cost for linear regression\n",
    "        J = COMPUTECOST(X, y, theta) computes the cost of using theta as the\n",
    "        parameter for linear regression to fit the data points in X and y\n",
    "        X, y, and theta are numpy arrays/matrices\n",
    "    '''\n",
    "\n",
    "    # Initialize some useful values\n",
    "    m = len(y) # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Compute the cost of a particular choice of theta\n",
    "    #               You should set J to the cost.\n",
    "\n",
    "\n",
    "    return J.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((np.ones((m,1)), x), axis=1) # Add a column of ones to x\n",
    "theta = np.zeros((2,1)) # initialize fitting parameters\n",
    "\n",
    "# Some gradient descent settings\n",
    "iterations = 1500\n",
    "alpha = 0.01\n",
    "\n",
    "# compute and display initial cost\n",
    "computeCost(X, y, theta)\n",
    "# You should expect to see a cost of 32.07."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    '''GRADIENTDESCENT Performs gradient descent to learn theta\n",
    "        theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by \n",
    "        taking num_iters gradient steps with learning rate alpha\n",
    "    '''\n",
    "\n",
    "    # Initialize some useful values\n",
    "    m = len(y) # number of training examples\n",
    "    J_history = np.zeros((num_iters, 1))\n",
    "\n",
    "    for iter in range(num_iters):\n",
    "\n",
    "        # ====================== YOUR CODE HERE ======================\n",
    "        # Instructions: Perform a single gradient step on the parameter vector theta. \n",
    "        # Hint: While debugging, it can be useful to print out the values\n",
    "        #       of the cost function (computeCost) and gradient here.\n",
    "\n",
    "        # ============================================================\n",
    "        # Save the cost J in every iteration    \n",
    "        J_history[iter] = computeCost(X, y, theta)\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gradient descent\n",
    "theta, J_history = gradientDescent(X, y, theta, alpha, iterations)\n",
    "\n",
    "# print theta to screen\n",
    "print('Theta found by gradient descent: ')\n",
    "print(theta[0], theta[1])\n",
    "\n",
    "# Plot again the values\n",
    "plt.plot(x, y, 'rx', markersize=10) #Plot the data \n",
    "plt.ylabel('Profit in $10,000s') #Set the y−axis label \n",
    "plt.xlabel('Population of City in 10,000s') # Set the x−axis label\n",
    "# Plot the linear fit\n",
    "plt.plot(X[:,1], np.dot(X,theta), '-')\n",
    "plt.legend('Training data', 'Linear regression')\n",
    "\n",
    "# Predict values for population sizes of 35,000 and 70,000\n",
    "predict1 = np.dot([1, 3.5],theta)\n",
    "print('For population = 35,000, we predict a profit of ', predict1*10000);\n",
    "predict2 = np.dot([1, 7],theta)\n",
    "print('For population = 70,000, we predict a profit of ', predict2*10000);\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Visualizing $J(\\theta)$\n",
    "To understand the cost function $J(\\theta)$ better, you will now plot the cost over a 2-dimensional grid of $\\theta_0$ and $\\theta_1$ values. You will not need to code anything new for this part, but you should understand how the code you have written already is creating these images."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Grid over which we will calculate J\n",
    "theta0_vals = np.linspace(-10, 10, 100)\n",
    "theta1_vals = np.linspace(-1, 4, 100)\n",
    "\n",
    "# initialize J_vals to a matrix of 0's\n",
    "J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))\n",
    "\n",
    "# Fill out J_vals\n",
    "for i in range(len(theta0_vals)):\n",
    "    for j in range(len(theta1_vals)):\n",
    "\t    t = np.array([theta0_vals[i], theta1_vals[j]])\n",
    "\t    J_vals[i,j] = computeCost(X, y, t.reshape(2,1))\n",
    "\n",
    "# Because of the way meshgrids work in the surf command, we need to \n",
    "# transpose J_vals before calling surf, or else the axes will be flipped\n",
    "J_vals = J_vals.T\n",
    "# Surface plot\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig,azim=-37.5, elev=30)\n",
    "ax.plot_surface(theta0_vals, theta1_vals, J_vals)\n",
    "ax.set_xlabel('theta_0')\n",
    "ax.set_ylabel('theta_1')\n",
    "\n",
    "# Contour plot\n",
    "# Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100\n",
    "plt.figure()\n",
    "plt.contour(theta0_vals, theta1_vals, J_vals, np.logspace(-2, 3, 20))\n",
    "plt.xlabel('theta_0')\n",
    "plt.ylabel('theta_1')\n",
    "plt.plot(theta[0], theta[1], 'rx', markersize=10, linewidth=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}